# ğŸ§  RÃ©gression multiple & neurone artificiel

Ce mini-projet pÃ©dagogique montre comment passer d'une **rÃ©gression linÃ©aire multiple** Ã  un **neurone artificiel**, Ã  l'aide d'un exemple simple basÃ© sur les performances d'Ã©tudiants.

## ğŸ¯ Objectif

PrÃ©dire le score dâ€™un Ã©tudiant (Ã©tudiant D) en fonction de deux paramÃ¨tres :

- â³ Temps dâ€™Ã©tude (`xâ‚ = 4`)
- ğŸ˜´ Temps de sommeil (`xâ‚‚ = 5`)

## ğŸ¯ DonnÃ©es utilisÃ©es

| Ã‰tudiant | Biais (1) | Ã‰tude (xâ‚) | Sommeil (xâ‚‚) | Score (y) |
|----------|-----------|------------|--------------|-----------|
| A        | 1         | 2          | 6            | 0.3       |
| B        | 1         | 3          | 7            | 0.5       |
| C        | 1         | 1          | 8            | 0.4       |

> ğŸ” La colonne `1` reprÃ©sente le **biais** (valeur constante permettant au modÃ¨le de sâ€™ajuster mÃªme si toutes les entrÃ©es sont nulles).

## ğŸ§® RÃ©gression linÃ©aire multiple

On cherche une relation linÃ©aire entre les variables d'entrÃ©e et le score :

\[
y = aâ‚ \cdot xâ‚ + aâ‚‚ \cdot xâ‚‚ + b
\]

Avec :

- `xâ‚` = temps dâ€™Ã©tude  

- `xâ‚‚` = temps de sommeil  

- `aâ‚`, `aâ‚‚` = poids associÃ©s Ã  chaque variable  

- `b` = biais (valeur fixe Ã  apprendre)

## ğŸ“ Formule des moindres carrÃ©s

Pour calculer les poids optimaux (`aâ‚`, `aâ‚‚`, `b`), on utilise une rÃ©solution matricielle :

\[
\theta = (X^T \cdot X)^{-1} \cdot X^T \cdot y
\]

En Python (avec NumPy) :

```python
theta_best = np.linalg.inv(X.T @ X) @ X.T @ y

âœ… Poids appris par le modÃ¨le

RÃ©sultat du calcul :

Poids appris (biais, Ã©tude, sommeil) : [-0.5  0.1  0.1]
Soit :

    aâ‚ = 0.1

    aâ‚‚ = 0.1

    b = -0.5

Le modÃ¨le apprend ces poids automatiquement Ã  partir des donnÃ©es, afin de minimiser lâ€™erreur de prÃ©diction (par la mÃ©thode des moindres carrÃ©s).

ğŸ‘‰ Pas besoin de les choisir Ã  la main !

ğŸ” PrÃ©diction pour lâ€™Ã©tudiant D

Lâ€™Ã©tudiant D a :

    xâ‚ = 4 heures dâ€™Ã©tude

    xâ‚‚ = 5 heures de sommeil

Calcul du score :

yD=0.1â‹…4+0.1â‹…5âˆ’0.5=0.4

ğŸ“Š Score prÃ©dit (avant activation) : 0.4

ğŸ–¼ï¸ Visualisation 3D de la rÃ©gression

Voici le plan de rÃ©gression 3D qui montre lâ€™influence conjointe du sommeil et de lâ€™Ã©tude sur le score :

![Heatmap](https://github.com/DIAPkuik237/regression-multiple-neurone/blob/master/heatmap(2).png)

ğŸ§  Passage au neurone artificiel

Reprenons la formule obtenue :

z=a1â‹…x1+a2â‹…x2+b

Pour en faire un neurone artificiel, on ajoute une fonction dâ€™activation. Ici, on utilise la fonction sigmoÃ¯de :

La fonction sigmoÃ¯de Ïƒ(z) est dÃ©finie par :

Ïƒ(z) = 1 / (1 + e^(-z))


ğŸ”® ProbabilitÃ© de rÃ©ussite (aprÃ¨s sigmoÃ¯de) : 0.5987

âœ… Ce rÃ©sultat peut Ãªtre interprÃ©tÃ© comme une probabilitÃ© de succÃ¨s de ~59.87% pour lâ€™Ã©tudiant D.

ğŸ§© Ce que Ã§a nous apprend

    âœ… La rÃ©gression multiple combine plusieurs variables via une somme pondÃ©rÃ©e.

    âœ… Le modÃ¨le apprend automatiquement les meilleurs poids Ã  partir des donnÃ©es.

    âœ… En ajoutant une fonction dâ€™activation (sigmoÃ¯de), on crÃ©e un neurone simple.

    ğŸš€ Cette idÃ©e est la base du machine learning et des rÃ©seaux de neurones.

ğŸ”— Ressources

    ğŸ”— Article complet sur Medium

    ğŸ’» Code source sur GitHub

    ğŸ§ª ExÃ©cuter sur Google Colab

    ğŸ“˜ Lire d'abord : Un premier pas vers lâ€™IA avec la rÃ©gression linÃ©aire simple

ğŸ‘¨â€ğŸ”¬ RÃ©alisÃ© par

Franck KOUEKAM â€“ autodidacte en IA & vulgarisateur

ğŸ“º ChaÃ®ne DIAP âˆ€ â€” DÃ©mystifier lâ€™Intelligence Artificielle & Python pour tout le monde



