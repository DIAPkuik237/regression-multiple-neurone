# üß† R√©gression multiple & neurone artificiel

Ce mini-projet p√©dagogique montre comment passer d'une **r√©gression lin√©aire multiple** √† un **neurone artificiel**, √† l'aide d'un exemple simple bas√© sur les performances d'√©tudiants.

## üéØ Objectif

Pr√©dire le score d‚Äôun √©tudiant (√©tudiant D) en fonction de deux param√®tres :

- ‚è≥ Temps d‚Äô√©tude (`x‚ÇÅ = 4`)
- üò¥ Temps de sommeil (`x‚ÇÇ = 5`)

## üéØ Donn√©es utilis√©es

| √âtudiant | Biais (1) | √âtude (x‚ÇÅ) | Sommeil (x‚ÇÇ) | Score (y) |
|----------|-----------|------------|--------------|-----------|
| A        | 1         | 2          | 6            | 0.3       |
| B        | 1         | 3          | 7            | 0.5       |
| C        | 1         | 1          | 8            | 0.4       |

> üîé La colonne `1` repr√©sente le **biais** (valeur constante permettant au mod√®le de s‚Äôajuster m√™me si toutes les entr√©es sont nulles).

## üßÆ R√©gression lin√©aire multiple

On cherche une relation lin√©aire entre les variables d'entr√©e et le score :

\[
y = a‚ÇÅ \cdot x‚ÇÅ + a‚ÇÇ \cdot x‚ÇÇ + b
\]

Avec :

- `x‚ÇÅ` = temps d‚Äô√©tude  

- `x‚ÇÇ` = temps de sommeil  

- `a‚ÇÅ`, `a‚ÇÇ` = poids associ√©s √† chaque variable  

- `b` = biais (valeur fixe √† apprendre)

## üìê Formule des moindres carr√©s

Pour calculer les poids optimaux (`a‚ÇÅ`, `a‚ÇÇ`, `b`), on utilise une r√©solution matricielle :

\[
\theta = (X^T \cdot X)^{-1} \cdot X^T \cdot y
\]

En Python (avec NumPy) :

```python

theta_best = np.linalg.inv(X.T @ X) @ X.T @ y

==> Poids appris par le mod√®le :

R√©sultat du calcul :

Poids appris (biais, √©tude, sommeil) : [-0.5  0.1  0.1]

Soit :
    a1 = 0.1
    a2 = 0.1
    b  = -0.5

Le mod√®le apprend ces poids automatiquement √† partir des donn√©es,
afin de minimiser l‚Äôerreur de pr√©diction (par la m√©thode des moindres carr√©s).

=> Pas besoin de les choisir √† la main !

------------------------------------------------------------

Pr√©diction pour l‚Äô√©tudiant D

L‚Äô√©tudiant D a :
    x1 = 4 heures d‚Äô√©tude
    x2 = 5 heures de sommeil

Calcul du score :
    yD = 0.1 * 4 + 0.1 * 5 - 0.5 = 0.4

Score pr√©dit (avant activation) : 0.4

------------------------------------------------------------

Visualisation 3D de la r√©gression

Voici le plan de r√©gression 3D qui montre l‚Äôinfluence conjointe du sommeil et de l‚Äô√©tude sur le score :

Image (Heatmap) : https://github.com/DIAPkuik237/regression-multiple-neurone/blob/master/heatmap(2).png

------------------------------------------------------------

Passage au neurone artificiel

On reprend la formule obtenue :
    z = a1 * x1 + a2 * x2 + b

Puis on applique une fonction d‚Äôactivation. Ici : la fonction sigmo√Øde

D√©finition :
    sigma(z) = 1 / (1 + e^(-z))

Application pour z = 0.4 :
    sigma(0.4) ‚âà 0.5987

Interpr√©tation :
    Probabilit√© de r√©ussite : environ 59.87 %

------------------------------------------------------------

Ce que √ßa nous apprend :

    - La r√©gression multiple combine plusieurs variables via une somme pond√©r√©e.
    - Le mod√®le apprend automatiquement les meilleurs poids √† partir des donn√©es.
    - En ajoutant une fonction d‚Äôactivation (sigmo√Øde), on cr√©e un neurone simple.
    - Cette id√©e est la base du machine learning et des r√©seaux de neurones.

------------------------------------------------------------

Ressources :

    - Article complet sur Medium
    - Code source sur GitHub : https://github.com/DIAPkuik237/regression-multiple-neurone
    - Ex√©cuter sur Google Colab
    - Lire d'abord : Un premier pas vers l‚ÄôIA avec la r√©gression lin√©aire simple

------------------------------------------------------------

Projet r√©alis√© par :

Franck KOUEKAM ‚Äì autodidacte en IA & vulgarisateur

Cha√Æne YouTube : DIAP ‚àÄ ‚Äî D√©mystifier l‚ÄôIntelligence Artificielle & Python pour tout le monde


