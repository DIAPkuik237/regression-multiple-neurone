# ğŸ“˜ RÃ©gression multiple & neurone artificiel

Ce mini-projet pÃ©dagogique montre comment passer d'une **rÃ©gression linÃ©aire multiple** Ã  un **neurone artificiel**, Ã  l'aide d'un exemple simple basÃ© sur les performances d'Ã©tudiants.

---

## ğŸ¯ Objectif

PrÃ©dire le score dâ€™un Ã©tudiant (Ã©tudiant D) en fonction de deux paramÃ¨tres :

- â³ Temps dâ€™Ã©tude (`x1 = 4`)
- ğŸ˜´ Temps de sommeil (`x2 = 5`)

---

## ğŸ“Š DonnÃ©es

| Ã‰tudiant | Temps d'Ã©tude (x1) | Temps de sommeil (x2) | Score (y) |
|----------|--------------------|------------------------|-----------|
| A        | 2                  | 6                      | 0.3       |
| B        | 3                  | 7                      | 0.5       |
| C        | 1                  | 8                      | 0.4       |
| D        | 4                  | 5                      | ?         |



---

## ğŸ§® RÃ©gression linÃ©aire multiple

On cherche une relation du type :
y = a1 * x1 + a2 * x2 + b

Avec :

- `x1` : temps d'Ã©tude  
- `x2` : temps de sommeil  
- `a1`, `a2` : poids appris pour chaque variable  
- `b` : biais (interception)

---

## ğŸ“ MÃ©thode des moindres carrÃ©s

Formule utilisÃ©e :
theta = (Xáµ— * X)^(-1) * Xáµ— * y


---

## ğŸ”¢ DonnÃ©es sous forme matricielle
### Matrice X (avec biais)

| Biais | Ã‰tude (xâ‚) | Sommeil (xâ‚‚) |
| ----- | ---------- | ------------ |
| 1     | 2          | 6            |
| 1     | 3          | 7            |
| 1     | 1          | 8            |

> ğŸ” On ajoute une colonne de `1` dans la matrice des donnÃ©es pour intÃ©grer le **biais** `b`.

### Vecteur y
| Ã‰tudiant | Score (y) |
| -------- | --------- |
| A        | 0.3       |
| B        | 0.5       |
| C        | 0.4       |



---

## ğŸ’» ImplÃ©mentation en Python
import numpy as np

X = np.array([
    [1, 2, 6],  # Ã‰tudiant A
    [1, 3, 7],  # Ã‰tudiant B
    [1, 1, 8]   # Ã‰tudiant C
])

y = np.array([0.3, 0.5, 0.4])

theta_best = np.linalg.inv(X.T @ X) @ X.T @ y
print(theta_best)



#### âœ… RÃ©sultat obtenu
Poids appris par le modÃ¨le (dans lâ€™ordre : biais, x1, x2) :

[-0.5  0.1  0.1]

Soit :

a1 = 0.1

a2 = 0.1

b = -0.5

## ğŸ” PrÃ©diction pour lâ€™Ã©tudiant D

Lâ€™Ã©tudiant D a :

x1 = 4

x2 = 5

Calcul :

yD = 0.1 * 4 + 0.1 * 5 - 0.5 = 0.4

## ğŸ–¼ï¸ Visualisation 3D de la rÃ©gression

Voici le plan de rÃ©gression qui modÃ©lise l'influence combinÃ©e du sommeil et du temps d'Ã©tude :
![Heatmap](https://github.com/DIAPkuik237/regression-multiple-neurone/blob/master/heatmap(2).png)


## ğŸ§  Passage au neurone artificiel

Formule de sortie du neurone :

z = a1 * x1 + a2 * x2 + b

Puis, on applique une fonction dâ€™activation.

Fonction sigmoÃ¯de :

Ïƒ(z) = 1 / (1 + e^(-z))

Application pour z = 0.4 :

Ïƒ(0.4) â‰ˆ 0.5987

#### âœ… InterprÃ©tation :

ProbabilitÃ© de rÃ©ussite de lâ€™Ã©tudiant D : ~59.87â€¯%


## ğŸ“Œ Conclusion

âœ… La rÃ©gression multiple apprend automatiquement les poids optimaux pour combiner plusieurs variables.

âœ… En ajoutant une fonction dâ€™activation, on transforme la sortie en probabilitÃ© â†’ on obtient un neurone artificiel simple.

    ğŸ§  Câ€™est le principe fondamental derriÃ¨re les rÃ©seaux de neurones et le machine learning moderne.

## ğŸ’¬ Et vous ?

Quelles autres variables pourraient influencer la rÃ©ussite dâ€™un Ã©tudiant ?

Le niveau de stress ?

La motivation personnelle ?

La qualitÃ© de lâ€™alimentation ?

Lâ€™environnement familial ?

ğŸ’¡ Et si vous intÃ©griez ces paramÃ¨tres dans un futur modÃ¨le ?

### ğŸ‘¨â€ğŸ”¬ Projet rÃ©alisÃ© par

Franck KOUEKAM â€” Autodidacte en IA & vulgarisateur
ChaÃ®ne YouTube : DIAP âˆ€ â€” DÃ©mystifier lâ€™IA & Python pour tout le monde












